{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_lines(path):\n",
    "    r\"\"\" Open text file by path and read all lines \"\"\"\n",
    "    with open(path, \"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "        \n",
    "    # transform docs into lists of words\n",
    "    raw_lines = [l.split() for l in lines]\n",
    "\n",
    "    return raw_lines\n",
    "\n",
    "def to_data_and_label(raw_lines):\n",
    "    r\"\"\" Split training data and label from raw lines \"\"\"\n",
    "    y = list(map(lambda x: int(x[0]), raw_lines))\n",
    "    x = list(map(lambda x: x[1:], raw_lines))\n",
    "    return (x, y)\n",
    "\n",
    "def save_result_as_file(prediction, file_name=\"prediction.dat\"):\n",
    "    r\"\"\" Save the predicted result as a new file \"\"\"\n",
    "    file_content = \"\\n\".join(list(map(str, prediction)))\n",
    "    with open(file_name, \"w\") as fd:\n",
    "        fd.write(file_content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Matrix Relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_term_ids(docs):\n",
    "    r\"\"\" The docs should be the combination of training set and test set.\"\"\"\n",
    "    term_ids = {}\n",
    "    curr_term_id = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in term_ids:\n",
    "                term_ids[w] = curr_term_id\n",
    "                curr_term_id += 1\n",
    "    return (term_ids, nnz)\n",
    "\n",
    "def build_sparse_matrix(docs, term_ids = {}, nnz = 0):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    ncols = len(term_ids)\n",
    "    assert(ncols != 0)\n",
    "\n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows + 1, dtype=np.int)\n",
    "    row_id = 0  # document ID / row counter\n",
    "    acc = 0  # non-zero counter\n",
    "\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k, _ in cnt.most_common())\n",
    "        curr_doc_len = len(keys)\n",
    "        for i, key in enumerate(keys):\n",
    "            ind[acc + i] = term_ids[key]\n",
    "            val[acc + i] = cnt[key]\n",
    "        ptr[row_id + 1] = ptr[row_id] + curr_doc_len\n",
    "        acc += curr_doc_len\n",
    "        row_id += 1\n",
    "\n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_l2normalize(mat, copy=False):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmer(name, k=3):\n",
    "    r\"\"\" Given a name and parameter k, return the vector of k-mers associated with the name\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    name_len = len(name)\n",
    "    v = []\n",
    "    if name_len < k:\n",
    "        return [name]\n",
    "\n",
    "    for i in range(name_len - k + 1):\n",
    "        v.append(name[i:i+k])\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(mat, cls, fold=1, d=10):\n",
    "    r\"\"\" Split the matrix and class info into train and test data using d-fold hold-out\n",
    "    \"\"\"\n",
    "    n = mat.shape[0]\n",
    "    r = int(np.ceil(n*1.0/d))\n",
    "    mattr = []\n",
    "    clstr = []\n",
    "    # split mat and cls into d folds\n",
    "    for f in range(d):\n",
    "        if f+1 != fold:\n",
    "            mattr.append( mat[f*r: min((f+1)*r, n)] )\n",
    "            clstr.extend( cls[f*r: min((f+1)*r, n)] )\n",
    "    # join all fold matrices that are not the test matrix\n",
    "    train = sp.vstack(mattr, format='csr')\n",
    "    # extract the test matrix and class values associated with the test rows\n",
    "    test = mat[(fold-1)*r: min(fold*r, n), :]\n",
    "    clste = cls[(fold-1)*r: min(fold*r, n)]\n",
    "\n",
    "    return train, clstr, test, clste\n",
    "\n",
    "def classifyNames(x_train, cls, c=3, k=3, folds=10):\n",
    "    def classify(x, train, clstr):\n",
    "        r\"\"\" Classify vector x using kNN and majority vote rule given training data and associated classes\n",
    "        \"\"\"\n",
    "        # find nearest neighbors for x\n",
    "        dots = x.dot(train.T)\n",
    "        sims = list(zip(dots.indices, dots.data))\n",
    "        if len(sims) == 0:\n",
    "            # could not find any neighbors\n",
    "            return 1 if np.random.rand() > 0.5 else -1\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        tc = Counter(clstr[s[0]] for s in sims[:k]).most_common(2)\n",
    "        if len(tc) < 2 or tc[0][1] > tc[1][1]:\n",
    "            # majority vote\n",
    "            return tc[0][0]\n",
    "        # tie break\n",
    "        tc = defaultdict(float)\n",
    "        for s in sims[:k]:\n",
    "            tc[clstr[s[0]]] += s[1]\n",
    "        return sorted(tc.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        \n",
    "    macc = 0.0\n",
    "    for f in range(folds):\n",
    "        # split data into training and testing\n",
    "        train, clstr, test, clste = splitData(x_train, cls, f+1, folds)\n",
    "        # predict the class of each test sample\n",
    "        clspr = [ classify(test[i,:], train, clstr) for i in range(test.shape[0]) ]\n",
    "        \n",
    "        macc += matthews_corrcoef(clste, clspr)\n",
    "        \n",
    "    return macc/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn():\n",
    "    # Read files\n",
    "    train_lines = load_data_as_lines(\"train.dat\")\n",
    "    x_test = load_data_as_lines(\"test.dat\")\n",
    "    # Split \n",
    "    x_train, y_train = to_data_and_label(train_lines)\n",
    "    \n",
    "    # Text preprocessing\n",
    "    print(\"Preprocessing documents...\")\n",
    "    docs_train = list(map(lambda x: kmer(x[0], k=3), x_train))\n",
    "    docs_test = list(map(lambda x: kmer(x[0], k=3), x_test))\n",
    "    \n",
    "    # Build sparse matrix\n",
    "    term_ids, nnz = calc_term_ids([*docs_train])\n",
    "    mat_train = build_sparse_matrix(docs_train, term_ids, nnz)\n",
    "    csr_l2normalize(mat_train)\n",
    "    \n",
    "    # Find best k\n",
    "    max_k = 500\n",
    "    curr_max_acc = 0\n",
    "    max_acc_k = 0\n",
    "    history = [[] for i in range(max_k)]\n",
    "\n",
    "    for k in range(max_k):\n",
    "        accuracy = classifyNames(mat_train, y_train, k=k+1)\n",
    "        history[k].append(accuracy)\n",
    "        if accuracy > curr_max_acc:\n",
    "            curr_max_acc = accuracy\n",
    "            max_acc_k = k + 1\n",
    "\n",
    "    print(\"k=%d, accuracy: %f\" % (max_acc_k, curr_max_acc))\n",
    "    \n",
    "    # Create prediction file\n",
    "    term_ids2, nnz2 = calc_term_ids([*docs_train, *docs_test])\n",
    "    mat_train2 = build_sparse_matrix(docs_train, term_ids2, nnz2)\n",
    "    mat_test2 = build_sparse_matrix(docs_test, term_ids2, nnz2)\n",
    "    csr_l2normalize(mat_train2)\n",
    "    csr_l2normalize(mat_test2)\n",
    "    \n",
    "    y_test = predict(mat_test2, mat_train2, y_train, k=max_acc_k)\n",
    "    file_name = \"knn%d-kmer3.dat\" % max_acc_k\n",
    "    save_result_as_file(y_test, file_name)\n",
    "    print(\"File %s created.\" % file_name)\n",
    "    \n",
    "def predict(test_data, train_data, labels, k=1):\n",
    "    r\"\"\" Predict the label of test data based on the given\n",
    "    labeled training data using k-nearest neighbor classifier.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    similarity = train_data.dot(test_data.T).todense()\n",
    "    top_k_idx = np.argpartition(similarity, -k, axis=0)[-k:,:]\n",
    "    to_labels = np.vectorize(lambda idx: labels[idx])\n",
    "    top_k_label = to_labels(top_k_idx)\n",
    "    \n",
    "    n_test_col = similarity.shape[1]\n",
    "\n",
    "    for col in range(n_test_col):\n",
    "        train_tags = top_k_label[:,col].flatten().tolist()[0]\n",
    "        train_indices = top_k_idx[:,col].flatten().tolist()[0]\n",
    "\n",
    "        # Select the maximum aggregated similarity\n",
    "        weights = {}\n",
    "        for tag, row in zip(train_tags, train_indices):\n",
    "            if tag in weights:\n",
    "                weights[tag][\"count\"] += 1\n",
    "                weights[tag][\"value\"] += similarity[row, col]\n",
    "            else:\n",
    "                weights[tag] = {\n",
    "                    \"count\": 1,\n",
    "                    \"value\": similarity[row, col]\n",
    "                }\n",
    "                \n",
    "        result = max(weights.items(), key=lambda x: (x[1][\"count\"], x[1][\"value\"]))[0]\n",
    "        predictions.append(result)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Other Models (SVC, Logictic Regression, Naive Bayes, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_cv(x_train, x_test=None):\n",
    "    count_vect = CountVectorizer()\n",
    "    docs_train_joined = list(map(lambda x: \" \".join(x), x_train))\n",
    "    docs_test_joined = None if x_test is None else \\\n",
    "        list(map(lambda x: \" \".join(x), x_test))\n",
    "\n",
    "    x_train_cv = count_vect.fit_transform(docs_train_joined)\n",
    "    x_test_cv = None if x_test is None else \\\n",
    "        count_vect.transform(docs_test_joined)\n",
    "        \n",
    "    return x_train_cv, x_test_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_data(x_train):\n",
    "    def percvar(v):\n",
    "        r\"\"\"Transform eigen/singular values into percents.\n",
    "        Return: vector of percents, prefix vector of percents\n",
    "        \"\"\"\n",
    "        # sort values\n",
    "        s = np.sort(np.abs(v))\n",
    "        # reverse sorting order\n",
    "        s = s[::-1]\n",
    "        # normalize\n",
    "        s = s/np.sum(s)\n",
    "        return s, np.cumsum(s)\n",
    "\n",
    "    def perck(s, p):\n",
    "        return next(i+1 for i,v in enumerate(s) if v >= p)\n",
    "\n",
    "    x_train_d = x_train.todense()\n",
    "    X_std = StandardScaler().fit_transform(x_train_d)\n",
    "    means = np.mean(X_std, axis=0)\n",
    "    X_sm = X_std - means\n",
    "    \n",
    "    U,s,V = np.linalg.svd(X_sm)\n",
    "    _, pv = percvar(s**2/(X_sm.shape[0]-1))\n",
    "\n",
    "    percentage_explained = 95\n",
    "    n_components = perck(pv, percentage_explained * 0.01)\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    svd.fit(x_train_d)\n",
    "\n",
    "    return svd.transform(x_train_d), svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_accuracy(data, y_train, use_pca=False, verbose=False):\n",
    "    models = [\n",
    "        LinearSVC(class_weight=\"balanced\"),\n",
    "        LinearSVC(),\n",
    "        LogisticRegression(class_weight=\"balanced\"),\n",
    "        LogisticRegression(random_state=0),\n",
    "        None if use_pca else MultinomialNB()\n",
    "    ]\n",
    "    models = list(filter(lambda x: x is not None, models))\n",
    "    folds = 10\n",
    "    scorer = make_scorer(matthews_corrcoef)\n",
    "    \n",
    "    data = data\n",
    "    res = []\n",
    "    for i, model in enumerate(models):\n",
    "        model_name = model.__class__.__name__\n",
    "        if model_name == \"LinearSVC\" and i == 0 or \\\n",
    "            model_name == \"LogisticRegression\" and i == 2:\n",
    "            model_name += \"(balanced)\"\n",
    "            \n",
    "        accuracies = cross_val_score(model, data, y_train, scoring=scorer, cv=folds)\n",
    "        result = (model_name, round(np.average(accuracies), 6))\n",
    "        if verbose:\n",
    "            print(result)\n",
    "        res.append(result)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    if model_name == \"LinearSVC\":\n",
    "        return LinearSVC()\n",
    "    if model_name == \"LinearSVC(balanced)\":\n",
    "        return LinearSVC(class_weight=\"balanced\")\n",
    "    if model_name == \"LogisticRegression\":\n",
    "        return LogisticRegression(random_state=0)\n",
    "    if model_name == \"LogisticRegression(balanced)\":\n",
    "        return LogisticRegression(class_weight=\"balanced\")\n",
    "    if model_name == \"MultinomialNB\":\n",
    "        return MultinomialNB()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(x_train, y_train, use_pca=False, verbose=False):\n",
    "    results = []\n",
    "    for k in range(2, 4):\n",
    "        print(\"k = %d\" % k)\n",
    "        \n",
    "        # Text preprocessing\n",
    "        if verbose:\n",
    "            print(\"Preprocessing documents...\")\n",
    "        docs_train = list(map(lambda x: kmer(x[0], k=k), x_train))\n",
    "\n",
    "        x_train_cv, _ = load_data_as_cv(docs_train)\n",
    "        if use_pca:\n",
    "            x_train_cv, _ = get_pca_data(x_train_cv)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Evaluating...\")\n",
    "        model_acc = get_models_accuracy(x_train_cv, y_train, use_pca, verbose)\n",
    "        model_acc = list(map(lambda x: (k, x[0], x[1]), model_acc))\n",
    "        \n",
    "        results = [*results, *model_acc]\n",
    "    \n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read files\n",
    "    train_lines = load_data_as_lines(\"train.dat\")\n",
    "    x_test = load_data_as_lines(\"test.dat\")\n",
    "\n",
    "    # Split \n",
    "    x_train, y_train = to_data_and_label(train_lines)\n",
    "    \n",
    "    # Get the most accurate models\n",
    "    res = test_models(x_train, y_train)[:10]\n",
    "    # Create files for accurate models\n",
    "    for k, model_name, _ in res:\n",
    "        docs_train = list(map(lambda x: kmer(x[0], k=k), x_train))\n",
    "        docs_test = list(map(lambda x: kmer(x[0], k=k), x_test))\n",
    "        x_train_cv, x_test_cv = load_data_as_cv(docs_train, docs_test)\n",
    "    \n",
    "        m = get_model(model_name)\n",
    "        m.fit(x_train_cv, y_train)\n",
    "        \n",
    "        y_test = m.predict(x_test_cv)\n",
    "        save_result_as_file(y_test, \"%s-%d.dat\" % (model_name, k))\n",
    "    \n",
    "    # Get the most accurate models with PCA\n",
    "    res_pca = test_models(x_train, y_train, use_pca=True)[:6]\n",
    "    # Create files for accurate PCA models\n",
    "    for k, model_name, _ in res_pca:\n",
    "        docs_train = list(map(lambda x: kmer(x[0], k=k), x_train))\n",
    "        docs_test = list(map(lambda x: kmer(x[0], k=k), x_test))\n",
    "        \n",
    "        x_train_cv, x_test_cv = load_data_as_cv(docs_train, docs_test)\n",
    "        x_train_cv, svd = get_pca_data(x_train_cv)\n",
    "        \n",
    "        x_test_cv = svd.transform(x_test_cv.todense())\n",
    "    \n",
    "        m = get_model(model_name)\n",
    "        m.fit(x_train_cv, y_train)\n",
    "        \n",
    "        y_test = m.predict(x_test_cv)\n",
    "        save_result_as_file(y_test, \"%s-%d_pca.dat\" % (model_name, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2\n",
      "k = 3\n",
      "k = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2\n",
      "k = 3\n",
      "k = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 'LogisticRegression', 0.853508),\n",
       " (2, 'LogisticRegression(balanced)', 0.828579),\n",
       " (3, 'LogisticRegression(balanced)', 0.827817),\n",
       " (3, 'LinearSVC(balanced)', 0.822402),\n",
       " (3, 'LinearSVC', 0.814257),\n",
       " (2, 'MultinomialNB', 0.812943),\n",
       " (2, 'LinearSVC', 0.803411),\n",
       " (2, 'LinearSVC(balanced)', 0.777075),\n",
       " (3, 'LogisticRegression', 0.772422),\n",
       " (4, 'LogisticRegression(balanced)', 0.73815),\n",
       " (4, 'LinearSVC', 0.723501),\n",
       " (4, 'LinearSVC(balanced)', 0.723244),\n",
       " (4, 'LogisticRegression', 0.651025),\n",
       " (3, 'MultinomialNB', 0.571121),\n",
       " (4, 'MultinomialNB', 0.106831)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 'LogisticRegression', 0.832498),\n",
       " (3, 'LinearSVC', 0.802863),\n",
       " (3, 'LinearSVC(balanced)', 0.793694),\n",
       " (2, 'LinearSVC', 0.779962),\n",
       " (3, 'LogisticRegression', 0.772219),\n",
       " (2, 'LinearSVC(balanced)', 0.757175),\n",
       " (4, 'LogisticRegression', 0.683727),\n",
       " (4, 'LinearSVC', 0.663933),\n",
       " (4, 'LinearSVC(balanced)', 0.58965)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_models(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
