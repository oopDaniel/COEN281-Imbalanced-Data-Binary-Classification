{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_lines(path):\n",
    "    r\"\"\" Open text file by path and read all lines \"\"\"\n",
    "    with open(path, \"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "        \n",
    "    # transform docs into lists of words\n",
    "    raw_lines = [l.split() for l in lines]\n",
    "\n",
    "    return raw_lines\n",
    "\n",
    "def to_data_and_label(raw_lines):\n",
    "    r\"\"\" Split training data and label from raw lines \"\"\"\n",
    "    y = list(map(lambda x: int(x[0]), raw_lines))\n",
    "    x = list(map(lambda x: x[1:], raw_lines))\n",
    "    return (x, y)\n",
    "\n",
    "def save_result_as_file(prediction, file_name=\"prediction.dat\"):\n",
    "    r\"\"\" Save the predicted result as a new file \"\"\"\n",
    "    file_content = \"\\n\".join(list(map(str, prediction)))\n",
    "    with open(file_name, \"w\") as fd:\n",
    "        fd.write(file_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmer(name, k=3):\n",
    "    r\"\"\" Given a name and parameter k, return the vector of k-mers associated with the name\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    name_len = len(name)\n",
    "    v = []\n",
    "    for i in range(name_len - k + 1):\n",
    "        v.append(name[i:i+k])\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse matrices, which requires aggregration of term IDs for both training and test data.\n",
    "\n",
    "def calc_term_ids(docs):\n",
    "    r\"\"\" The docs should be the combination of training set and test set.\"\"\"\n",
    "    term_ids = {}\n",
    "    curr_term_id = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in term_ids:\n",
    "                term_ids[w] = curr_term_id\n",
    "                curr_term_id += 1\n",
    "    return (term_ids, nnz)\n",
    "\n",
    "def build_sparse_matrix(docs, term_ids = {}, nnz = 0):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    ncols = len(term_ids)\n",
    "    assert(ncols != 0)\n",
    "\n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows + 1, dtype=np.int)\n",
    "    row_id = 0  # document ID / row counter\n",
    "    acc = 0  # non-zero counter\n",
    "\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k, _ in cnt.most_common())\n",
    "        curr_doc_len = len(keys)\n",
    "        for i, key in enumerate(keys):\n",
    "            ind[acc + i] = term_ids[key]\n",
    "            val[acc + i] = cnt[key]\n",
    "        ptr[row_id + 1] = ptr[row_id] + curr_doc_len\n",
    "        acc += curr_doc_len\n",
    "        row_id += 1\n",
    "\n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_l2normalize(mat, copy=False):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "train_lines = load_data_as_lines(\"train.dat\")\n",
    "# Split \n",
    "x_train, y_train = to_data_and_label(train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "x_test = load_data_as_lines(\"test.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing documents...\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "print(\"Preprocessing documents...\")\n",
    "docs_train = list(map(lambda x: kmer(x[0], k=3), x_train))\n",
    "docs_test = list(map(lambda x: kmer(x[0], k=3), x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ids, nnz = calc_term_ids([*docs_train])\n",
    "mat_train = build_sparse_matrix(docs_train, term_ids, nnz)\n",
    "csr_l2normalize(mat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(mat, cls, fold=1, d=10):\n",
    "    r\"\"\" Split the matrix and class info into train and test data using d-fold hold-out\n",
    "    \"\"\"\n",
    "    n = mat.shape[0]\n",
    "    r = int(np.ceil(n*1.0/d))\n",
    "    mattr = []\n",
    "    clstr = []\n",
    "    # split mat and cls into d folds\n",
    "    for f in range(d):\n",
    "        if f+1 != fold:\n",
    "            mattr.append( mat[f*r: min((f+1)*r, n)] )\n",
    "            clstr.extend( cls[f*r: min((f+1)*r, n)] )\n",
    "    # join all fold matrices that are not the test matrix\n",
    "    train = sp.vstack(mattr, format='csr')\n",
    "    # extract the test matrix and class values associated with the test rows\n",
    "    test = mat[(fold-1)*r: min(fold*r, n), :]\n",
    "    clste = cls[(fold-1)*r: min(fold*r, n)]\n",
    "\n",
    "    return train, clstr, test, clste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x, train, clstr):\n",
    "    r\"\"\" Classify vector x using kNN and majority vote rule given training data and associated classes\n",
    "    \"\"\"\n",
    "    # find nearest neighbors for x\n",
    "    dots = x.dot(train.T)\n",
    "    sims = list(zip(dots.indices, dots.data))\n",
    "    if len(sims) == 0:\n",
    "        # could not find any neighbors\n",
    "        return '+' if np.random.rand() > 0.5 else '-'\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    tc = Counter(clstr[s[0]] for s in sims[:k]).most_common(2)\n",
    "    if len(tc) < 2 or tc[0][1] > tc[1][1]:\n",
    "        # majority vote\n",
    "        return tc[0][0]\n",
    "    # tie break\n",
    "    tc = defaultdict(float)\n",
    "    for s in sims[:k]:\n",
    "        tc[clstr[s[0]]] += s[1]\n",
    "    return sorted(tc.items(), key=lambda x: x[1], reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNames(cls, c=3, k=3, d=10):\n",
    "    def classify(x, train, clstr):\n",
    "        r\"\"\" Classify vector x using kNN and majority vote rule given training data and associated classes\n",
    "        \"\"\"\n",
    "        # find nearest neighbors for x\n",
    "        dots = x.dot(train.T)\n",
    "        sims = list(zip(dots.indices, dots.data))\n",
    "        if len(sims) == 0:\n",
    "            # could not find any neighbors\n",
    "            return 1 if np.random.rand() > 0.5 else -1\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        tc = Counter(clstr[s[0]] for s in sims[:k]).most_common(2)\n",
    "        if len(tc) < 2 or tc[0][1] > tc[1][1]:\n",
    "            # majority vote\n",
    "            return tc[0][0]\n",
    "        # tie break\n",
    "        tc = defaultdict(float)\n",
    "        for s in sims[:k]:\n",
    "            tc[clstr[s[0]]] += s[1]\n",
    "        return sorted(tc.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        \n",
    "    macc = 0.0\n",
    "    for f in range(d):\n",
    "        # split data into training and testing\n",
    "        train, clstr, test, clste = splitData(mat_train, cls, f+1, d)\n",
    "        # predict the class of each test sample\n",
    "        clspr = [ classify(test[i,:], train, clstr) for i in range(test.shape[0]) ]\n",
    "        \n",
    "        macc += matthews_corrcoef(clste, clspr)\n",
    "        \n",
    "    return macc/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c=0, k=20, accuracy: 0.698850\n"
     ]
    }
   ],
   "source": [
    "max_c = 4\n",
    "max_k = 500\n",
    "\n",
    "max_acc = 0\n",
    "max_acc_c = 0\n",
    "max_acc_k = 0\n",
    "history = [[] for i in range(max_k)]\n",
    "\n",
    "# for c in range(max_c):\n",
    "#     for k in range(max_k):\n",
    "#         accuracy = classifyNames(names, cls, c=c+1, k=k+1)\n",
    "#         history[c].append(accuracy)\n",
    "#         if accuracy > max_acc:\n",
    "#             max_acc = accuracy\n",
    "#             max_acc_c = c + 1\n",
    "#             max_acc_k = k + 1\n",
    "\n",
    "\n",
    "for k in range(max_k):\n",
    "    accuracy = classifyNames(y_train, k=k+1)\n",
    "    history[k].append(accuracy)\n",
    "    if accuracy > max_acc:\n",
    "        max_acc = accuracy\n",
    "        max_acc_k = k + 1\n",
    "            \n",
    "print(\"c=%d, k=%d, accuracy: %f\" % (max_acc_c, max_acc_k, max_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ids2, nnz2 = calc_term_ids([*docs_train, *docs_test])\n",
    "mat_train2 = build_sparse_matrix(docs_train, term_ids2, nnz2)\n",
    "mat_test2 = build_sparse_matrix(docs_test, term_ids2, nnz2)\n",
    "csr_l2normalize(mat_train2)\n",
    "csr_l2normalize(mat_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, train_data, labels, k=1):\n",
    "    r\"\"\" Predict the label of test data based on the given\n",
    "    labeled training data using k-nearest neighbor classifier.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    similarity = train_data.dot(test_data.T).todense()\n",
    "    top_k_idx = np.argpartition(similarity, -k, axis=0)[-k:,:]\n",
    "    to_labels = np.vectorize(lambda idx: labels[idx])\n",
    "    top_k_label = to_labels(top_k_idx)\n",
    "    \n",
    "    n_test_col = similarity.shape[1]\n",
    "\n",
    "    for col in range(n_test_col):\n",
    "        train_tags = top_k_label[:,col].flatten().tolist()[0]\n",
    "        train_indices = top_k_idx[:,col].flatten().tolist()[0]\n",
    "\n",
    "        # Select the maximum aggregated similarity\n",
    "        weights = {}\n",
    "        for tag, row in zip(train_tags, train_indices):\n",
    "            if tag in weights:\n",
    "                weights[tag][\"count\"] += 1\n",
    "                weights[tag][\"value\"] += similarity[row, col]\n",
    "            else:\n",
    "                weights[tag] = {\n",
    "                    \"count\": 1,\n",
    "                    \"value\": similarity[row, col]\n",
    "                }\n",
    "                \n",
    "        result = max(weights.items(), key=lambda x: (x[1][\"count\"], x[1][\"value\"]))[0]\n",
    "        predictions.append(result)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = predict(mat_test2, mat_train2, y_train, k=20)\n",
    "save_result_as_file(y_test, \"knn20-kmer3.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
